





#Basics
import pandas as pd
import geopandas as gpd

#Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder

#Shapely
from geopy.geocoders import Nominatim
from shapely.geometry import box
from shapely import wkt
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry import mapping
from shapely.geometry import shape

#Plots!
import matplotlib.pyplot as plt

#Spatial Indexing
import h3

#Warning Supression
import warnings








#Skip the first six rows because they're header information
empl_data = pd.read_csv('data/employment_data/lsoa_by_industry_london.csv', skiprows=7, delimiter=',')

unnamed_cols = empl_data.columns[empl_data.columns.str.contains('^Unnamed:')]
empl_data.drop(columns=unnamed_cols, inplace=True)

#Separate name into LSOA11CD and LSOA11NM
def split_column(value):
    #Keep Greater London stats
    if value.startswith('gor:'):
        return value, value
    #Split into name and code
    else:
        parts = value.split('lsoa2011:')[1]
        code, name = parts.split(' : ')
        return code.strip(), name.strip()
        return code, name

empl_data[['LSOA11CD', 'LSOA11NM']] = empl_data['Area'].apply(lambda x: pd.Series(split_column(x)))

print("Num Rows (LSOAs) Before Cleaning: " + str(empl_data.shape[0]))

#There appear to be a bunch of duplicates so I'm going to get rid of them now
empl_data.drop_duplicates(inplace=True)

print("Num Rows (LSOAs) After Cleaning: " + str(empl_data.shape[0]))


#Create Total Column
empl_data['total'] = empl_data[empl_data.columns[1:-2]].sum(axis=1)

empl_data.head()





#Get London LSOA Shapefile Data
lsoa_geo = gpd.read_file('data/lsoa_data/LSOA_2011_London_gen_MHW.shp')

#Convert to WGS for consistency
lsoa_geo = lsoa_geo.to_crs(epsg=4326)
lsoa_geo = lsoa_geo.drop(lsoa_geo.columns[list(range(3, 8))], axis = 1)

print("Num Rows (LSOAs): " + str(lsoa_geo.shape[0]))

#Get rid of columns I'm not using for now
lsoa_geo = lsoa_geo.drop(columns=['MSOA11CD', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ'])
lsoa_geo = lsoa_geo.rename(columns={'USUALRES': 'population'})

lsoa_geo.head()





overture_places = gpd.read_file('data/overture_data/london_places_overture.geojson')

print('Number of POIs in Overture (London): ' + str(overture_places.shape[0]))
overture_places.head()





overture_buildings = gpd.read_file('data/overture_data/london_buildings_overture.geojson')

print('Number of Buildings in Overture (London): ' + str(overture_buildings.shape[0]))
overture_buildings.head()





osm_buildings = gpd.read_file('data/osm_data/all_london_buildings.geojson')

print('Number of Buildings in OSM (London): ' + str(osm_buildings.shape[0]))
osm_buildings.head()








#Note that total London statistics are dropped here because there's no equivalent column in lsoa_geo
empl_geog = pd.merge(lsoa_geo, empl_data, on = "LSOA11CD")

print("Num Rows (LSOAS): " + str(empl_geog.shape[0]))

empl_geog.head()





#FYI this cell takes kind of a long time - I should probably split it up because it does a lot

#Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

#Iterate through all LSOAs and grab OSM + Overture building counts - add to empl_geog dataframe

for index, row in empl_geog.iterrows():
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs=lsoa_geo.crs)
    
    #All Buildings - Overture
    filtered_overture_buildings = overture_buildings[overture_buildings.geometry.intersects(filter_geom.unary_union)]

    #Multipolygon of each LSOA's (Overture) buildings
    combined_overture_multipolygon = filtered_overture_buildings.geometry.unary_union

    
    
    #All Buildings - OSM
    filtered_osm_buildings = osm_buildings[osm_buildings.geometry.intersects(filter_geom.unary_union)]

    #Multipolygon of each LSOA's (OSM) buildings
    combined_osm_multipolygon = filtered_osm_buildings.geometry.unary_union


    
    #All Places
    filtered_places = overture_places[overture_places.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places['category'].dropna().tolist()
    

    #Add back to Dataframe
    empl_geog.at[index, 'num_overture_buildings'] = len(filtered_overture_buildings)
    empl_geog.at[index, 'num_osm_buildings'] = len(filtered_osm_buildings)
    empl_geog.at[index, 'num_places'] = len(filtered_places)
    empl_geog.at[index, 'overture_building_poly'] = combined_overture_multipolygon
    empl_geog.at[index, 'osm_building_poly'] = combined_osm_multipolygon
    empl_geog.at[index, 'place_points'] = combined_multipoint
    empl_geog.at[index, 'category_list'] = category_list
    
#Check
empl_geog.head()





#Get building subset - City of London 001F

#Code from earlier - isolate City of London 001F
lsoa = empl_geog[empl_geog['LSOA11NM_x'] == 'City of London 001F']
geom = lsoa['geometry'].iloc[0]
filter_geom = gpd.GeoSeries([geom], crs=lsoa_geo.crs)

#Get buildings for both
lsoa_osm_buildings = gpd.GeoDataFrame(lsoa, geometry='osm_building_poly')
lsoa_overture_buildings = gpd.GeoDataFrame(lsoa, geometry='overture_building_poly')

# Plot OSM buildings
fig, ax = plt.subplots(figsize=(20, 20))
lsoa_osm_buildings.plot(ax=ax, legend=True, color='blue', edgecolor='black')
filter_geom.boundary.plot(ax=ax, color='red', linewidth=2)
ax.set_axis_off()
plt.show()

# Plot Overture buildings
fig, ay = plt.subplots(figsize=(20, 20))
lsoa_overture_buildings.plot(ax=ay, legend=True, color='green', edgecolor='black')
filter_geom.boundary.plot(ax=ay, color='red', linewidth=2)
ay.set_axis_off()
plt.show()


#Get all London

# Plot OSM buildings
fig, ax = plt.subplots(figsize=(20, 20))
osm_buildings.plot(ax=ax, legend=True, color='blue', edgecolor='black')
ax.set_axis_off()
plt.show()








#Before incorporating any more data into my model, I want to see how well COUNTS alone can predict employment at LSOA level

#Identifying variables (feature variables vs target to predict)
features = ['num_overture_buildings', 'num_places', 'population']
target = 'total'

X = empl_geog[features]
y = empl_geog[target]

#Split into train/test - I know I need to make this spatially sensitive but we're just going with this for now
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Init/train model
model = LinearRegression()
model.fit(X_train, y_train)

#Predictions
y_pred = model.predict(X_test)

#Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")





#Before incorporating any more data into my model, I want to see how well COUNTS alone can predict employment at LSOA level

#Identifying variables (feature variables vs target to predict)
features = ['num_osm_buildings', 'num_places', 'population']
target = 'total'

X = empl_geog[features]
y = empl_geog[target]

#Split into train/test - I know I need to make this spatially sensitive but we're just going with this for now
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Init/train model
model = LinearRegression()
model.fit(X_train, y_train)

#Predictions
y_pred = model.predict(X_test)

#Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")





# Encode POI categories in data - using One-Hot encoding

#Explode category lists
empl_geog_exploded = empl_geog.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse=False)

#Encode
encoded_categories = encoder.fit_transform(empl_geog_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_exploded = pd.concat([empl_geog_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist() + ['num_osm_buildings', 'num_overture_buildings', 'num_places', 'population', 'total']
empl_geog_encoded = empl_geog_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

empl_geog_encoded.head()





#TODO - FIX THIS UP FROM ITS EARLIER VERSION - LOTS OF VARIABLE NAMES NEED TO BE CHANGED

# Function to extract slightly more information about the building footprints contained in each LSOA
def extract_multipolygon_features(multipolygon):
    if isinstance(multipolygon, MultiPolygon):
        polygons = list(multipolygon.geoms)
    elif isinstance(multipolygon, Polygon):
        polygons = [multipolygon]
    else:
        return pd.Series({
            'num_polygons': 0,
            'total_area': 0,
            'total_perimeter': 0,
            'avg_area': 0,
            'avg_perimeter': 0
        })

    num_polygons = len(polygons)
    areas = [polygon.area for polygon in polygons]
    perimeters = [polygon.length for polygon in polygons]
    total_area = sum(areas)
    total_perimeter = sum(perimeters)
    avg_area = total_area / num_polygons if num_polygons > 0 else 0
    avg_perimeter = total_perimeter / num_polygons if num_polygons > 0 else 0

    return pd.Series({
        'num_polygons': num_polygons,
        'total_area': total_area,
        'total_perimeter': total_perimeter,
        'avg_area': avg_area,
        'avg_perimeter': avg_perimeter
    })


geometry_features = empl_geog['building_poly'].apply(extract_multipolygon_features)
empl_geog = pd.concat([empl_geog, geometry_features], axis=1)

