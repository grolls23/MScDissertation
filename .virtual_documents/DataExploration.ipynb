





# Library Imports

#Basics
import pandas as pd
import geopandas as gpd
import numpy as np

#Shapely
from shapely import wkt
import shapely.geometry
from shapely.geometry import Polygon, MultiPolygon

#Plots and Stats
import matplotlib.pyplot as plt
import seaborn as sns

#Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.preprocessing import OneHotEncoder

#ML from mljar-supervised
from supervised.automl import AutoML

#Warning Supression
import warnings


# Import Cleaned Employment Geography File

empl_geog = gpd.read_file('data/combined_data/empl_geog.geojson')

# WKT strings back to Shapely geometries
empl_geog['overture_building_poly'] = empl_geog['overture_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['osm_building_poly'] = empl_geog['osm_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['place_points'] = empl_geog['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

# Convert string representations of lists back to lists
empl_geog['category_list'] = empl_geog['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

empl_geog.head()


# Import Category Encoded File

encoded_categories = pd.read_csv('data/combined_data/empl_geog_category_encoded.csv')

#Drop extraneous column
encoded_categories = encoded_categories.drop(columns=['Unnamed: 0'])

#Store category columns
category_columns = encoded_categories.columns[1:]

encoded_categories.head()


# Geometric Features Extraction (copied from earlier)

def extract_multipolygon_features(multipolygon, lsoa_geometry):
    if isinstance(multipolygon, MultiPolygon):
        polygons = list(multipolygon.geoms)
    elif isinstance(multipolygon, Polygon):
        polygons = [multipolygon]
    else:
        return pd.Series({
            'num_polygons': 0,
            'total_area': 0,
            'total_perimeter': 0,
            'area_ratio': 0,
            'avg_area': 0,
            'avg_perimeter': 0
        })

    num_polygons = len(polygons)
    areas = [polygon.area for polygon in polygons]
    perimeters = [polygon.length for polygon in polygons]
    total_area = sum(areas)
    total_perimeter = sum(perimeters)
    avg_area = total_area / num_polygons if num_polygons > 0 else 0
    avg_perimeter = total_perimeter / num_polygons if num_polygons > 0 else 0

    #Built-up area ratio could result in a div by zero error if there are no buildings in an LSOA so this logic has to be included here
    try:
        area_ratio = total_area / lsoa_geometry.area
    except ZeroDivisionError:
        area_ratio = 0

    return pd.Series({
        'num_polygons': num_polygons,
        'total_area': total_area,
        'area_ratio': area_ratio,
        'total_perimeter': total_perimeter,
        'avg_area': avg_area,
        'avg_perimeter': avg_perimeter
    })



#Apply function above

#Both Building Set Geometry Features
osm_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['osm_building_poly'], row['geometry']),
    axis=1
)

empl_geog_osm_features = pd.concat([empl_geog, osm_geometry_features], axis=1)


# Combine Datasets (Same Logic as in EarlyModels.ipynb)

all_data = pd.merge(empl_geog_osm_features, encoded_categories, on='LSOA11CD')

all_data.head()








# To do this, I'll need to redownload the London Overture POIs because I didn't carry the confidence data forward

all_places = gpd.read_file('data/overture_data/london_places_overture.geojson')

all_places.head()


# Extract Confidence Scores & Plot

confidence_scores = all_places['confidence']

plt.hist(confidence_scores, bins=30, edgecolor='black')
plt.title('Histogram of Confidence Scores (London)')
plt.xlabel('Confidence Score')
plt.ylabel('Frequency')
plt.show()


# Cleaning Dataframe by removing POIs with under 0.6 confidence score

quality_places = all_places[all_places['confidence'] >= 0.6]

#Calculate % Remaining
uncleaned_poi_count = all_places.shape[0]
cleaned_poi_count = quality_places.shape[0]

print('Percent of POIs above 0.6 Confidence: ' + str(100 * cleaned_poi_count / uncleaned_poi_count))


#Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Generating a version of empl_geog that has only quality places

empl_geog_quality_places = empl_geog_osm_features.drop(columns = ['category_list'])

#Copy back quality places using logic from DataCleaning.ipynb

empl_geog_quality_places['category_list'] = None

for index, row in empl_geog.iterrows():
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs='EPSG:4326')

    #All Places
    filtered_places = quality_places[quality_places.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places['category'].dropna().tolist()

    #Add back to Dataframe
    empl_geog_quality_places.at[index, 'category_list'] = category_list

empl_geog_quality_places.head()


# Encoding (Similar to EarlyModelling.ipynb)

#Explode category lists
empl_geog_quality_places_exploded = empl_geog_quality_places.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories_quality = encoder.fit_transform(empl_geog_quality_places_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories_quality, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_quality_places_exploded = pd.concat([empl_geog_quality_places_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_quality_places_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_quality_places_encoded = empl_geog_quality_places_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Save columns of interest for analysis

quality_categories = empl_geog_quality_places_encoded.columns[1:]

empl_geog_quality_places_encoded.head()



# Rejoin with main dataframe

all_data_quality_places = pd.merge(empl_geog_quality_places, empl_geog_quality_places_encoded, on='LSOA11CD')

all_data_quality_places.head()





# Most Common Categories

#Get totals
category_totals = encoded_categories.sum().drop('LSOA11CD')

#Sort by most common
category_totals = category_totals.sort_values(ascending=False)

category_types = [
    'restaurant',
    'services',
    'store',
    'shop',
    'market',
    'park',
    'center',
    'school',
    'museum',
    'hospital',
    'clinic',
    'office',
    'gym',
    'church',
    'station',
    'salon',
    'stadium',
    'manufacturer',
]

# Check and print the count for each category type
for category_type in category_types:
    columns = category_totals.index.str.lower().str.endswith(category_type)
    num_columns = columns.sum()
    print(f'Number of {category_type.capitalize()} Categories: {num_columns}')

print('\nExamples:')
print(category_totals.head(10))


# Automated Condensing of Column Types

# List all columns
all_columns = encoded_categories.columns.tolist()

# I'll group them by 'suffix' to find commonalities
suffix_groups = {}

# Iterate through columns to identify suffixes
for column in all_columns:
    lower_column = column.lower()
    parts = lower_column.split('_')
    if len(parts) > 1:
        suffix = parts[-1]
        if suffix in suffix_groups:
            suffix_groups[suffix].append(column)
        else:
            suffix_groups[suffix] = [column]
    else:
        # Handle columns without suffixes
        suffix_groups['other'] = suffix_groups.get('other', []) + [column]

# Create a list of DataFrames to concatenate
concatenated_dfs = []

# Iterate through suffix groups and aggregate columns
for suffix, columns in suffix_groups.items():
    if columns:
        if len(columns) == 1:
            # Preserve col name if only one col with suffix
            new_column_name = columns[0].replace('category_list_', '')
            concatenated_dfs.append(encoded_categories[columns].rename(columns={columns[0]: new_column_name}))
        else:
            # Aggregate columns with more than one column in the group
            new_column_name = f'all_{suffix}'
            concatenated_dfs.append(encoded_categories[columns].sum(axis=1).rename(new_column_name))

# Concatenate all DataFrames
condensed_categories = pd.concat(concatenated_dfs, axis=1)

# Print summary information (groups only)
for suffix, columns in suffix_groups.items():
    num_columns = len(columns)
    if num_columns > 1:
        print(f'Number of {suffix.capitalize()} Categories: {num_columns}')

condensed_categories.head()


#Remove all categories that have less than five in London (arbitrary cutoff for now)

column_sums = condensed_categories.iloc[:, 1:].sum()
columns_to_drop = column_sums[column_sums < 5].index.tolist()
print(columns_to_drop)

condensed_categories = condensed_categories.drop(columns=columns_to_drop)

#Store new categories
condensed_category_columns = condensed_categories.columns[1:]

condensed_categories.head()


# Join Condensed Version with data (Same Logic as in EarlyModels.ipynb and above)

all_data_condensed_cat = pd.merge(empl_geog_osm_features, condensed_categories, on='LSOA11CD')

all_data_condensed_cat.head()





# I want to take a look at the different types of employment captured in BRES to see if we can't target specific subsets

job_cols = all_data_condensed_cat.columns[4:92]

print(job_cols)

# These are the most likely to be office work
office_work_cols = [
    '42 : Civil engineering',
    '58 : Publishing activities',
    '59 : Motion picture, video and television programme production, sound recording and music publishing activities',
    '60 : Programming and broadcasting activities',
    '61 : Telecommunications',
    '62 : Computer programming, consultancy and related activities',
    '63 : Information service activities',
    '64 : Financial service activities, except insurance and pension funding',
    '65 : Insurance, reinsurance and pension funding, except compulsory social security',
    '66 : Activities auxiliary to financial services and insurance activities',
    '68 : Real estate activities',
    '69 : Legal and accounting activities',
    '70 : Activities of head offices; management consultancy activities',
    '71 : Architectural and engineering activities; technical testing and analysis',
    '72 : Scientific research and development',
    '73 : Advertising and market research',
    '74 : Other professional, scientific and technical activities',
    '77 : Rental and leasing activities',
    '78 : Employment activities',
    '79 : Travel agency, tour operator and other reservation service and related activities',
    '80 : Security and investigation activities',
    '82 : Office administrative, office support and other business support activities',
    '84 : Public administration and defence; compulsory social security'
]


# Create a new Office Work Total:

all_data_condensed_cat['office_total'] = all_data_condensed_cat[office_work_cols].sum(axis=1)

all_data_condensed_cat.head()

#And do it for quality places only
all_data_quality_places['office_total'] = all_data_quality_places[office_work_cols].sum(axis=1)





%matplotlib inline 
# Colinearity Matrix - Gonna exclude employment and POI categories in this one because there are SO many

# Choose my columns
selected_columns = [
    'population',
    'num_osm_buildings',
    'num_places',
    'num_polygons',
    'total_area',
    'total_perimeter',
    'area_ratio',
    'avg_area',
    'avg_perimeter'
]

colinearity_features = empl_geog_osm_features[selected_columns]

plt.figure(figsize=(10,7))

# Heatmap
sns.heatmap(colinearity_features.corr(), annot=True, mask=mask, vmin=-1, vmax=1)
plt.title('Correlation Between LSOA Features')
plt.show()



# Test Collinearity on Categorical Variables???

%matplotlib inline 

# Choose my columns
selected_columns = [
    'population',
    'num_osm_buildings',
    'num_places',
    'num_polygons',
    'total_area',
    'total_perimeter',
    'area_ratio',
    'avg_area',
    'avg_perimeter',
    'all_cafe',
    'all_restaurant',
    'all_service',
]

colinearity_features = all_data_condensed_cat[selected_columns]

plt.figure(figsize=(10,7))

# Heatmap
sns.heatmap(colinearity_features.corr(), annot=True, vmin=-1, vmax=1)
plt.title('Correlation Between LSOA Features - Common Categories Included')
plt.show()






# PCA Stuff Here









# create training and testing data
features = ['num_polygons', 'total_area', 'total_perimeter', 'area_ratio', 'avg_area', 'avg_perimeter', 'num_osm_buildings', 'num_places', 'population'] + list(category_columns)
target = 'total'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data[features], all_data[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results/first_model/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
print(f'R^2 Score: {r2}')






#AutoML Test - Collapsed Categories

# create training and testing data
features = ['num_polygons', 'total_area', 'total_perimeter', 'area_ratio', 'avg_area', 'avg_perimeter', 'num_osm_buildings', 'num_places', 'population'] + list(condensed_category_columns)
target = 'total'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_condensed_cat[features], all_data_condensed_cat[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results/category_combined/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
print(f'R^2 Score: {r2}')






# Because LSOAs differ so greatly in employment counts, I will create a new employment density metric and try and run a model based off of that

#This is employees per hectare
#I'm also reprojecting my LSOA areas to British National Grid so I can get accurate area statistics
all_data_condensed_cat['employment_density'] = all_data_condensed_cat['total'] / (all_data_condensed_cat['geometry']).to_crs("EPSG:27700").area

all_data_condensed_cat.head()


# Model with Density as the Target

# create training and testing data
features = ['num_polygons', 'total_area', 'total_perimeter', 'area_ratio', 'avg_area', 'avg_perimeter', 'num_osm_buildings', 'num_places', 'population'] + list(condensed_category_columns)
target = 'employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_condensed_cat[features], all_data_condensed_cat[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results/category_combined_empl_density/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
print(f'R^2 Score: {r2}')






# Model with Cleaned POIs (No Category Groupings)

# create training and testing data
features = ['num_polygons', 'total_area', 'total_perimeter', 'area_ratio', 'avg_area', 'avg_perimeter', 'num_osm_buildings', 'num_places', 'population'] + list(quality_categories)
target = 'total'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_quality_places[features], all_data_quality_places[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results/quality_places_initial/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
print(f'R^2 Score: {r2}')






# create training and testing data
features = ['num_polygons', 'total_area', 'total_perimeter', 'area_ratio', 'avg_area', 'avg_perimeter', 'num_osm_buildings', 'num_places', 'population'] + list(quality_categories)
target = 'office_total'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_quality_places[features], all_data_quality_places[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results/quality_places_office_only/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
print(f'R^2 Score: {r2}')






# create training and testing data
features = ['num_polygons', 'total_area', 'total_perimeter', 'area_ratio', 'avg_area', 'avg_perimeter', 'num_osm_buildings', 'num_places', 'population'] + list(condensed_category_columns)
target = 'office_total'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_condensed_cat[features], all_data_condensed_cat[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results/all_pois_office_only/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
print(f'R^2 Score: {r2}')









fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_condensed_cat.plot(ax=ax, column='total', legend=True, cmap='viridis')
plt.title('Employment Count Map of London LSOAs')
ax.set_axis_off()
plt.title('Employment Count Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/employment_count_map.png')
plt.close(fig)





fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_condensed_cat.plot(ax=ax, column='employment_density', legend=True, cmap='viridis')
plt.title('Employment Density Map of London LSOAs')
ax.set_axis_off()
plt.title('Employment Density Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/employment_density_map.png')
plt.close(fig)





# Derive Office Employment Density

all_data_condensed_cat['office_employment_density'] = all_data_condensed_cat['office_total'] / (all_data_condensed_cat['geometry']).to_crs("EPSG:27700").area

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_condensed_cat.plot(ax=ax, column='office_employment_density', legend=True, cmap='viridis')
plt.title('Office Employment Density Map of London LSOAs')
ax.set_axis_off()
plt.title('Office Employment Density Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/office_employment_density_map.png')
plt.close(fig)





# Derive Overture POI Density

all_data_condensed_cat['poi_density'] = all_data_condensed_cat['num_places'] / (all_data_condensed_cat['geometry']).to_crs("EPSG:27700").area

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_condensed_cat.plot(ax=ax, column='poi_density', legend=True, cmap='viridis')
plt.title('Overture POI Density Map of London LSOAs')
ax.set_axis_off()
plt.title('Overture POI Density Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/poi_density_map.png')
plt.close(fig)





fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_condensed_cat.plot(ax=ax, column='num_places', legend=True, cmap='viridis')
plt.title('Overture POI Count Map of London LSOAs')
ax.set_axis_off()
plt.title('Overture POI Count Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/poi_count_map.png')
plt.close(fig)
