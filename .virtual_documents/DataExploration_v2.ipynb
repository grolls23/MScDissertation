





# Library Imports

#Basics
import pandas as pd
import geopandas as gpd
import numpy as np

#Shapely
from shapely import wkt
import shapely.geometry
from shapely.geometry import Polygon, MultiPolygon

#Plots and Stats
import matplotlib.pyplot as plt
import seaborn as sns

#Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

#ML from mljar-supervised
from supervised.automl import AutoML

#Warning Supression
import warnings


# Import Cleaned Employment Geography File (Version 2)

empl_geog = gpd.read_file('data/combined_data_v2/empl_geog.geojson')

# WKT strings back to Shapely geometries
empl_geog['building_poly'] = empl_geog['building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['commercial_building_poly'] = empl_geog['commercial_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['retail_building_poly'] = empl_geog['retail_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['office_building_poly'] = empl_geog['office_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['residential_building_poly'] = empl_geog['residential_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

empl_geog['place_points'] = empl_geog['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
# Convert string representations of lists back to lists
empl_geog['category_list'] = empl_geog['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

empl_geog.head()


# Import Category Encoded File (POIs)

encoded_categories = pd.read_csv('data/combined_data/empl_geog_category_encoded.csv')

#Drop extraneous column
encoded_categories = encoded_categories.drop(columns=['Unnamed: 0'])

#Store category columns
category_columns = encoded_categories.columns[1:]

encoded_categories.head()


# Geometric Features Extraction (copied from earlier but includes building types now)

#I'm going to exclude num polygons, average perimeter, and total perimeter cause they're not so helpful (cause Collinearity matrix shows its very close to count)

def extract_multipolygon_features(multipolygon, lsoa_geometry):
    if isinstance(multipolygon, MultiPolygon):
        polygons = list(multipolygon.geoms)
    elif isinstance(multipolygon, Polygon):
        polygons = [multipolygon]
    else:
        return pd.Series({
            'total_area': 0,
            'avg_building_area': 0,
            'lsoa_area_ratio': 0,
        })

    num_polygons = len(polygons)
    areas = [polygon.area for polygon in polygons]
    total_area = sum(areas)
    avg_building_area = total_area / num_polygons if num_polygons > 0 else 0

    #Built-up area ratio could result in a div by zero error if there are no buildings in an LSOA so this logic has to be included here
    try:
        lsoa_area_ratio = total_area / lsoa_geometry.area
    except ZeroDivisionError:
        lsoa_area_ratio = 0

    return pd.Series({
        'total_area': total_area,
        'lsoa_area_ratio': lsoa_area_ratio,
        'avg_building_area': avg_building_area,
    })



# All Buildings
all_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['building_poly'], row['geometry']),
    axis=1
).add_prefix('all_')

# Residential
residential_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['residential_building_poly'], row['geometry']),
    axis=1
).add_prefix('residential_')

# Commercial
commercial_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['commercial_building_poly'], row['geometry']),
    axis=1
).add_prefix('commercial_')

# Office
office_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['office_building_poly'], row['geometry']),
    axis=1
).add_prefix('office_')

# Retail
retail_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['retail_building_poly'], row['geometry']),
    axis=1
).add_prefix('retail_')

# Combine all geometry features into one DataFrame
all_geom_features = pd.concat([
    all_buildings_geometry_features,
    residential_buildings_geometry_features,
    commercial_buildings_geometry_features,
    office_buildings_geometry_features,
    retail_buildings_geometry_features
], axis=1, ignore_index=False)

# Capture Column Names
geo_features = all_geom_features.columns

# Display the first 50 rows
all_geom_features.head(50)


# Combine Datasets

data_with_geom = pd.concat([empl_geog, all_geom_features], axis=1)
all_data = pd.concat([data_with_geom, encoded_categories], axis=1)

all_data.head()





# create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(category_columns)
target = 'total_employment'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data[features], all_data[target], test_size=0.2, random_state=3)

# Save results and fit
automl = AutoML(results_path="automl_results_v2/basic_building_categories/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')


#Save results for plotting
predictions_all = automl.predict(all_data[features])
geometries = all_data_cleaned.loc[all_data[target].index, 'geometry']

results_basic_building = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data[target],
    'predicted': predictions_all,
})






# This work is currently storedin DataExploration.ipynb - some adaptation will be needed to migrate things over

all_places = gpd.read_file('data/overture_data/london_places_overture.geojson')
all_places.head()


#Filter to quality places only
quality_places = all_places[all_places['confidence'] >= 0.6]


#Also copied from original DataExploration file

#Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Generating a version of empl_geog that has only quality places - I'm using my data_with_geom file as a starting point
empl_geog_quality_places = data_with_geom

#Copy back quality places using logic from DataCleaning.ipynb

empl_geog_quality_places['category_list'] = None

for index, row in empl_geog.iterrows():
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs='EPSG:4326')

    #All Places
    filtered_places = quality_places[quality_places.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places['category'].dropna().tolist()

    #Add back to Dataframe
    empl_geog_quality_places.at[index, 'category_list'] = category_list

empl_geog_quality_places.head()


# One-Hot Encoding - yet again

#Explode category lists
empl_geog_quality_places_exploded = empl_geog_quality_places.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories_quality = encoder.fit_transform(empl_geog_quality_places_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories_quality, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_quality_places_exploded = pd.concat([empl_geog_quality_places_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_quality_places_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_quality_places_encoded = empl_geog_quality_places_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Save columns of interest for analysis

quality_categories = empl_geog_quality_places_encoded.columns[1:]

empl_geog_quality_places_encoded.head()



#Rejoin with all data

all_data_quality_places = pd.merge(empl_geog_quality_places, empl_geog_quality_places_encoded, on='LSOA11CD')

all_data_quality_places.head()


# Collapse Categories (Also Copied from DataExploration)

# Automated Condensing of Column Types

# List all columns
all_columns = encoded_categories.columns.tolist()

# I'll group them by 'suffix' to find commonalities
suffix_groups = {}

# Iterate through columns to identify suffixes
for column in all_columns:
    lower_column = column.lower()
    parts = lower_column.split('_')
    if len(parts) > 1:
        suffix = parts[-1]
        if suffix in suffix_groups:
            suffix_groups[suffix].append(column)
        else:
            suffix_groups[suffix] = [column]
    else:
        # Handle columns without suffixes
        suffix_groups['other'] = suffix_groups.get('other', []) + [column]

# Create a list of DataFrames to concatenate
concatenated_dfs = []

# Iterate through suffix groups and aggregate columns
for suffix, columns in suffix_groups.items():
    if columns:
        if len(columns) == 1:
            # Preserve col name if only one col with suffix
            new_column_name = columns[0].replace('category_list_', '')
            concatenated_dfs.append(encoded_categories[columns].rename(columns={columns[0]: new_column_name}))
        else:
            # Aggregate columns with more than one column in the group
            new_column_name = f'all_{suffix}'
            concatenated_dfs.append(encoded_categories[columns].sum(axis=1).rename(new_column_name))

# Concatenate all DataFrames
condensed_categories = pd.concat(concatenated_dfs, axis=1)

# Print summary information (groups only)
for suffix, columns in suffix_groups.items():
    num_columns = len(columns)
    if num_columns > 1:
        print(f'Number of {suffix.capitalize()} Categories: {num_columns}')

condensed_categories.head()


#Remove all categories that have less than five in London (arbitrary cutoff for now)

column_sums = condensed_categories.iloc[:, 1:].sum()
columns_to_drop = column_sums[column_sums < 5].index.tolist()
print(columns_to_drop)

condensed_categories = condensed_categories.drop(columns=columns_to_drop)

#Store new categories
condensed_category_columns = condensed_categories.columns[1:]

condensed_categories.head()


# Join Condensed Version with data (Same Logic as in EarlyModels.ipynb and above)

all_data_cleaned = pd.merge(data_with_geom, condensed_categories, on='LSOA11CD')

all_data_cleaned.head()







# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'total_employment'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results_v2/raw_employment_category_cleaned/", mode='Explain')

# fit the model
automl.fit(X_train, y_train)


predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_raw_employment_category_cleaned = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})






# Create a new column Employment Density and create new model to target that
all_data_cleaned['employment_density'] = all_data_cleaned['total_employment'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area

# Also creating my log employment count for plotting here because it didn't get carried over from the other file for ~some reason~
all_data_cleaned['log_total_employment'] = np.log(all_data_cleaned['total_employment'] + 1)

# Same with num_places
all_data_cleaned['log_num_places'] = np.log(all_data_cleaned['num_places'] + 1)



# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results_v2/employment_density_category_cleaned/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_empl_density = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})






# Use office work cols from previous analysis

office_work_cols = [
    '42 : Civil engineering',
    '58 : Publishing activities',
    '59 : Motion picture, video and television programme production, sound recording and music publishing activities',
    '60 : Programming and broadcasting activities',
    '61 : Telecommunications',
    '62 : Computer programming, consultancy and related activities',
    '63 : Information service activities',
    '64 : Financial service activities, except insurance and pension funding',
    '65 : Insurance, reinsurance and pension funding, except compulsory social security',
    '66 : Activities auxiliary to financial services and insurance activities',
    '68 : Real estate activities',
    '69 : Legal and accounting activities',
    '70 : Activities of head offices; management consultancy activities',
    '71 : Architectural and engineering activities; technical testing and analysis',
    '72 : Scientific research and development',
    '73 : Advertising and market research',
    '74 : Other professional, scientific and technical activities',
    '77 : Rental and leasing activities',
    '78 : Employment activities',
    '79 : Travel agency, tour operator and other reservation service and related activities',
    '80 : Security and investigation activities',
    '82 : Office administrative, office support and other business support activities',
    '84 : Public administration and defence; compulsory social security'
]

# Create a new Office Work Total Column :
all_data_cleaned['office_total_employment'] = all_data_cleaned[office_work_cols].sum(axis=1)

# And an Office Work Density Column
all_data_cleaned['office_employment_density'] = all_data_cleaned['office_total_employment'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area

all_data_cleaned.head()



# Run the model

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'office_employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

# Save results and fit
automl = AutoML(results_path="automl_results_v2/office_employment_density_category_cleaned/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_density_cleaned = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})






# Performance Model Raw Employment

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'total_employment'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

automl = AutoML(
    results_path ='automl_results_v2/employment_cleaned_perform/',
    algorithms=["CatBoost", "Xgboost", "Random Forest"],
    model_time_limit=1*60,
    start_random_models=5,
    hill_climbing_steps=3,
    top_models_to_improve=3,
    features_selection=False,
    stack_models=True,
    train_ensemble=True,
    explain_level=1,
    validation_strategy={
        "validation_type": "kfold",
        "k_folds": 4,
        "shuffle": False,
        "stratify": True,
    }
)

# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_cleaned_perform = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})


# Performance Model Employment Density

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

automl = AutoML(
    results_path ='automl_results_v2/employment_density_cleaned_perform/',
    algorithms=["CatBoost", "Xgboost", "Random Forest"],
    model_time_limit=1*60,
    start_random_models=5,
    hill_climbing_steps=3,
    top_models_to_improve=3,
    features_selection=False,
    stack_models=True,
    train_ensemble=True,
    explain_level=1,
    validation_strategy={
        "validation_type": "kfold",
        "k_folds": 4,
        "shuffle": False,
        "stratify": True,
    }
)

# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_density_cleaned_perform = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})


# Performance Model Office Employment Density

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'office_employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

automl = AutoML(
    results_path ='automl_results_v2/office_employment_density_cleaned_perform/',
    algorithms=["CatBoost", "Xgboost", "Random Forest"],
    model_time_limit=1*60,
    start_random_models=5,
    hill_climbing_steps=3,
    top_models_to_improve=3,
    features_selection=False,
    stack_models=True,
    train_ensemble=True,
    explain_level=1,
    validation_strategy={
        "validation_type": "kfold",
        "k_folds": 4,
        "shuffle": False,
        "stratify": True,
    }
)

# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_density_cleaned_perform = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})





# London Residual Plots

# Basic Model

results_basic_building = gpd.GeoDataFrame(results_basic_building, geometry='geometry')
results_basic_building['residual'] = results_basic_building['observed'] - results_basic_building['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_basic_building.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - London Basic Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_london/basic_model_residuals.png')
plt.close(fig)

# Category Cleaned Performance (Not Office)

results_office_cleaned_perform = gpd.GeoDataFrame(results_office_cleaned_perform, geometry='geometry')
results_office_cleaned_perform['residual'] = results_office_cleaned_perform['observed'] - results_office_cleaned_perform['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_office_cleaned_perform.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - London Category Cleaned Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_london/category_cleaned_model_performance_residuals.png')
plt.close(fig)


# Category Cleaned Employment Density

results_empl_density = gpd.GeoDataFrame(results_empl_density, geometry='geometry')
results_empl_density['residual'] = results_empl_density['observed'] - results_empl_density['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_empl_density.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - London Employment Density Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_london/density_model_residuals.png')
plt.close(fig)

# Category Cleaned Employment Density Office Employment

results_office_density_cleaned = gpd.GeoDataFrame(results_office_density_cleaned, geometry='geometry')
results_office_density_cleaned['residual'] = results_office_density_cleaned['observed'] - results_office_density_cleaned['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_office_density_cleaned.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - London Office Employment Density Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_london/office_density_model_residuals.png')
plt.close(fig)

# Category Cleaned

results_raw_employment_category_cleaned = gpd.GeoDataFrame(results_raw_employment_category_cleaned, geometry='geometry')
results_raw_employment_category_cleaned['residual'] = results_raw_employment_category_cleaned['observed'] - results_raw_employment_category_cleaned['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_raw_employment_category_cleaned.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - London Category Cleaned Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_london/category_cleaned_model_residuals.png')
plt.close(fig)





# Employment Count
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='total_employment', legend=True, cmap='viridis')
plt.title('Employment Count Map of London LSOAs')
ax.set_axis_off()
plt.title('Employment Count Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/employment_count_map.png')
plt.close(fig)

# Log Employment Count
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='log_total_employment', legend=True, cmap='viridis')
plt.title('(log) Employment Count Map of London LSOAs')
ax.set_axis_off()
plt.title('(log) Employment Count Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/log_employment_count_map.png')
plt.close(fig)

# Employment Density
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='employment_density', legend=True, cmap='viridis')
plt.title('Employment Density Map of London LSOAs')
ax.set_axis_off()
plt.title('Employment Density Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/employment_density_map.png')
plt.close(fig)

# Office Employment Density
all_data_cleaned['office_employment_density'] =all_data_cleaned['office_total_employment'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='office_employment_density', legend=True, cmap='viridis')
plt.title('Office Employment Density Map of London LSOAs')
ax.set_axis_off()
plt.title('Office Employment Density Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/office_employment_density_map.png')
plt.close(fig)

# POI density
all_data_cleaned['poi_density'] = all_data_cleaned['num_places'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='poi_density', legend=True, cmap='viridis')
plt.title('Overture POI Density Map of London LSOAs')
ax.set_axis_off()
plt.title('Overture POI Density Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/poi_density_map.png')
plt.close(fig)

# POI Counts
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='num_places', legend=True, cmap='viridis')
plt.title('Overture POI Count Map of London LSOAs')
ax.set_axis_off()
plt.title('Overture POI Count Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/poi_count_map.png')
plt.close(fig)

# Log POI Counts
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='log_num_places', legend=True, cmap='viridis')
plt.title('(log) Overture POI Count Map of London LSOAs')
ax.set_axis_off()
plt.title('(log) Overture POI Count Map of London LSOAs')
plt.savefig('Plots/from_code/stats_london/log_poi_count_map.png')
plt.close(fig)


