








#Library Imports - using DuckDB for Overture Import

#Database
import duckdb

#Basics
import pandas as pd
import geopandas as gpd
import numpy as np
from geopy.geocoders import Nominatim

#OpenStreetMap
import osmnx as ox

#Shapely
from geopy.geocoders import Nominatim
from shapely.geometry import box
from shapely import wkt
import shapely.geometry
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry import mapping
from shapely.geometry import shape

#Plots
import matplotlib.pyplot as plt
import seaborn as sns

#Warning Supression
import warnings

#Machine Learning
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
import ast

#ML from mljar-supervised
from supervised.automl import AutoML

#Warning Supression
import warnings


#Config SQL
%pip install ipython-sql duckdb duckdb-engine jupysql --quiet
%pip install --upgrade grpcio --quiet
%load_ext sql


%config SqlMagic.autopandas = True
%config SqlMagic.feedback = False
%config SqlMagic.displaycon = False
%sql duckdb:///:memory:


%%sql      
INSTALL httpfs;

LOAD httpfs;

INSTALL spatial;

LOAD spatial;

SET s3_region='us-west-2';





#Get Birmingham Bounding Box

# Initialize the geolocator
geolocator = Nominatim(user_agent="geoapi")

# Get location data
location = geolocator.geocode("Birmingham")

# Get the bounding box
bounding_box = location.raw['boundingbox']

# Convert bounding box to coordinates
min_lat, max_lat = float(bounding_box[0]), float(bounding_box[1])
min_lon, max_lon = float(bounding_box[2]), float(bounding_box[3])

print(min_lon)
print(min_lat)
print(max_lon)
print(max_lat)


%%sql

LOAD azure;

SET azure_storage_connection_string = 'DefaultEndpointsProtocol=https;AccountName=overturemapswestus2;AccountKey=;EndpointSuffix=core.windows.net';
COPY (
SELECT
    names.primary as primary_name,
    height,
    level,
    ST_GeomFromWKB(geometry) as geometry
FROM read_parquet('azure://release/2024-05-16-beta.0/theme=buildings/type=building/*', filename=true, hive_partitioning=1)
WHERE primary_name IS NOT NULL
AND bbox.xmin > -2.0336486
AND bbox.xmax < -1.7288417
AND bbox.ymin > 52.381053
AND bbox.ymax < 52.6087058
) TO 'data/overture_data/bham_buildings_overture.geojson'
WITH (FORMAT GDAL, DRIVER 'GeoJSON', SRS 'EPSG:4326');






%%sql

COPY (
    SELECT
        names.primary AS name,
        categories.main as category,
        ROUND(confidence,2) as confidence,
        ST_GeomFromWKB(geometry) as geometry
FROM read_parquet('s3://overturemaps-us-west-2/release/2024-05-16-beta.0/theme=places/*/*')
WHERE
    bbox.xmin BETWEEN -2.0336486 AND -1.7288417 AND
    bbox.ymin BETWEEN 52.381053 AND 52.6087058
) TO 'data/overture_data/bham_places_overture.geojson' WITH (FORMAT GDAL, DRIVER 'GeoJSON', SRS 'EPSG:4326');





#Basic overview stats of Birmingham Data - Using the locally saved files here
buildings = gpd.read_file('data/overture_data/bham_buildings_overture.geojson')
places = gpd.read_file('data/overture_data/bham_places_overture.geojson')

#Count of Features
print("Birmingham Building Count: " + str(buildings.shape[0]))
print("Birmingham POI count: " + str(places.shape[0]))





#Download Data for Birmingham
place_name = 'Birmingham, United Kingdom'

#Test new config
ox.config(use_cache=True, log_console=True)

buildings = ox.features_from_place(place_name, tags={'building': True})
buildings = buildings[buildings.geometry.notnull()]
building_footprints = buildings[buildings.geom_type.isin(['Polygon', 'MultiPolygon'])]

for col in building_footprints.columns:
    if building_footprints.apply(lambda x: isinstance(x, list)).any():
        building_footprints = building_footprints[col].apply(lambda x: str(x) if isinstance(x, list) else x)

building_footprints = building_footprints[['name', 'geometry']].reset_index()
print(building_footprints)
        
# Save the combined GeoDataFrame to a geojson file
building_footprints.to_file("data/osm_data/bham_buildings.geojson", driver="GeoJSON")





#Download Data for Birmingham
place_name = 'Birmingham, United Kingdom'

commercial_buildings = ox.features_from_place(place_name, tags={'building': ['commercial']})

commercial_buildings = commercial_buildings[commercial_buildings.geometry.notnull()]
commercial_building_footprints = commercial_buildings[commercial_buildings.geom_type.isin(['Polygon', 'MultiPolygon'])]

for col in commercial_building_footprints.columns:
    if commercial_building_footprints.apply(lambda x: isinstance(x, list)).any():
        commercial_building_footprints = commercial_building_footprints[col].apply(lambda x: str(x) if isinstance(x, list) else x)

commercial_building_footprints = commercial_building_footprints[['name', 'geometry']].reset_index()
print(commercial_building_footprints)
        
# Save the combined GeoDataFrame to a geojson file
commercial_building_footprints.to_file("data/osm_data/bham_commercial_buildings.geojson", driver="GeoJSON")





#Download Data for Birmingham
place_name = 'Birmingham, United Kingdom'

ox.config(use_cache=True, log_console=True)

office_buildings = ox.features_from_place(place_name, tags={'building': ['office']})

office_buildings = office_buildings[office_buildings.geometry.notnull()]
office_building_footprints = office_buildings[office_buildings.geom_type.isin(['Polygon', 'MultiPolygon'])]

for col in office_building_footprints.columns:
    if office_building_footprints.apply(lambda x: isinstance(x, list)).any():
        office_building_footprints[col] = office_building_footprints[col].apply(lambda x: str(x) if isinstance(x, list) else x)

office_building_footprints = office_building_footprints[['name', 'geometry']].reset_index()
print(office_building_footprints)
        
# Save the combined GeoDataFrame to a geojson file
office_building_footprints.to_file("data/osm_data/bham_office_buildings.geojson", driver="GeoJSON")






#Download Data for Birmingham
place_name = 'Birmingham, United Kingdom'

ox.config(use_cache=True, log_console=True)

residential_buildings = ox.features_from_place(place_name, tags={'building': ['residential']})

residential_buildings = residential_buildings[residential_buildings.geometry.notnull()]
residential_building_footprints = residential_buildings[residential_buildings.geom_type.isin(['Polygon', 'MultiPolygon'])]

for col in residential_building_footprints.columns:
    if residential_building_footprints.apply(lambda x: isinstance(x, list)).any():
        residential_building_footprints[col] = residential_building_footprints[col].apply(lambda x: str(x) if isinstance(x, list) else x)

residential_building_footprints = residential_building_footprints[['name', 'geometry']].reset_index()
print(residential_building_footprints)
        
# Save the combined GeoDataFrame to a geojson file
residential_building_footprints.to_file("data/osm_data/bham_residential_buildings.geojson", driver="GeoJSON")






#Download Data for Birmingham
place_name = 'Birmingham, United Kingdom'

ox.config(use_cache=True, log_console=True)

retail_buildings = ox.features_from_place(place_name, tags={'building': ['retail']})

retail_buildings = retail_buildings[retail_buildings.geometry.notnull()]
retail_building_footprints = retail_buildings[retail_buildings.geom_type.isin(['Polygon', 'MultiPolygon'])]

for col in retail_building_footprints.columns:
    if retail_building_footprints.apply(lambda x: isinstance(x, list)).any():
        retail_building_footprints[col] = retail_building_footprints[col].apply(lambda x: str(x) if isinstance(x, list) else x)

retail_building_footprints = retail_building_footprints[['name', 'geometry']].reset_index()
print(retail_building_footprints)
        
# Save the combined GeoDataFrame to a geojson file
retail_building_footprints.to_file("data/osm_data/bham_retail_buildings.geojson", driver="GeoJSON")









#Skip the first six rows because they're header information
empl_data = pd.read_csv('data/employment_data/lsoa_by_industry.csv', skiprows=7, delimiter=',')

unnamed_cols = empl_data.columns[empl_data.columns.str.contains('^Unnamed:')]
empl_data.drop(columns=unnamed_cols, inplace=True)

#Separate name into LSOA11CD and LSOA11NM
def split_column(value):
    if isinstance(value, str) and 'lsoa2011:' in value:
        parts = value.split('lsoa2011:')[1]
        code, name = parts.split(' : ')
        return code.strip(), name.strip()
    else:
        return None, None

empl_data[['LSOA11CD', 'LSOA11NM']] = empl_data['Area'].apply(lambda x: pd.Series(split_column(x)))

# Drop rows not in Birmingham
empl_data.dropna(subset=['LSOA11NM'], inplace=True)
empl_data = empl_data[empl_data['LSOA11NM'].str.contains('Birmingham')]

print("Num Rows (LSOAs) Before Cleaning: " + str(empl_data.shape[0]))

#There appear to be a bunch of duplicates so I'm going to get rid of them now
empl_data.drop_duplicates(inplace=True)

print("Num Rows (LSOAs) After Cleaning: " + str(empl_data.shape[0]))


#Create Total Employment Column
empl_data[empl_data.columns[1:-2]] = empl_data[empl_data.columns[1:-2]].apply(pd.to_numeric, errors='coerce')
empl_data['total_employment'] = empl_data[empl_data.columns[1:-2]].sum(axis=1)

empl_data.head()

max_index = empl_data['total_employment'].idxmax()
max_row = empl_data.loc[max_index]
print(max_row)





#Get LSOA Shapefile Data
lsoa_geo = gpd.read_file('data/lsoa_data/LSOA_2011_EW_BFE_V3.shp')

#Convert to WGS for consistency
lsoa_geo = lsoa_geo.to_crs(epsg=4326)

print("Num Rows (LSOAs): " + str(lsoa_geo.shape[0]))

#Get rid of columns I'm not using for now
lsoa_geo = lsoa_geo.drop(columns=['BNG_E', 'BNG_N', 'LONG_', 'LAT', 'GlobalID', 'Shape_Leng'])

#Join with population
lsoa_pop = pd.read_csv('data/lsoa_data/lsoa_pop.csv')

lsoa_geo.drop(columns=['LSOA11NM'])

#Get population estimates - had to be pulled in from separate Census dataset
lsoa_geo = lsoa_geo.merge(lsoa_pop, on='LSOA11CD')

lsoa_geo.head()





overture_places = gpd.read_file('data/overture_data/bham_places_overture.geojson')

print('Number of POIs in Overture (Birmingham): ' + str(overture_places.shape[0]))
overture_places.head()





osm_all_buildings = gpd.read_file('data/osm_data/bham_buildings.geojson')

print('Number of Buildings in OSM (Birmingham): ' + str(osm_all_buildings.shape[0]))
osm_all_buildings.head()


# Commercial Buildings

osm_commercial_buildings = gpd.read_file('data/osm_data/bham_commercial_buildings.geojson')

print('Number of Commercial Buildings in OSM (Birmingham): ' + str(osm_commercial_buildings.shape[0]))
osm_commercial_buildings.head()


# Office Buildings

osm_office_buildings = gpd.read_file('data/osm_data/bham_office_buildings.geojson')

print('Number of Office Buildings in OSM (Birmingham): ' + str(osm_office_buildings.shape[0]))
osm_office_buildings.head()


# Residential Buildings

osm_residential_buildings = gpd.read_file('data/osm_data/bham_residential_buildings.geojson')

print('Number of Residential Buildings in OSM (Birmingham): ' + str(osm_residential_buildings.shape[0]))
osm_residential_buildings.head()


# Retail Buildings

osm_retail_buildings = gpd.read_file('data/osm_data/bham_retail_buildings.geojson')

print('Number of Retail Buildings in OSM (Birmingham): ' + str(osm_retail_buildings.shape[0]))
osm_retail_buildings.head()


# Assign a building type in the main DataFrame if the osmid is found in one of the other DataFrames

# I know that office and retail buildings could have commercial or office but office and retail will overwrite commercia
# (and are thus checked after) because they're more specific

osm_all_buildings['building_type'] = 'none'

osm_all_buildings.loc[osm_all_buildings['osmid'].isin(osm_commercial_buildings['osmid']), 'building_type'] = 'commercial'
osm_all_buildings.loc[osm_all_buildings['osmid'].isin(osm_retail_buildings['osmid']), 'building_type'] = 'retail'
osm_all_buildings.loc[osm_all_buildings['osmid'].isin(osm_office_buildings['osmid']), 'building_type'] = 'office'
osm_all_buildings.loc[osm_all_buildings['osmid'].isin(osm_residential_buildings['osmid']), 'building_type'] = 'residential'

osm_all_buildings.head(50)








#Note that total Birmingham statistics are dropped here because there's no equivalent column in lsoa_geo
empl_geog = pd.merge(lsoa_geo, empl_data, on = "LSOA11CD")

print("Num Rows (LSOAS): " + str(empl_geog.shape[0]))

empl_geog.head()





#Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

#Init Columns (This was causing an error at first when I didn't put it here to begin)
empl_geog['num_buildings'] = 0
empl_geog['num_retail_buildings'] = 0
empl_geog['num_residential_buildings'] = 0
empl_geog['num_commercial_buildings'] = 0
empl_geog['num_office_buildings'] = 0

empl_geog['num_places'] = 0

empl_geog['building_poly'] = None
empl_geog['commercial_building_poly'] = None
empl_geog['retail_building_poly'] = None
empl_geog['office_building_poly'] = None
empl_geog['residential_building_poly'] = None

empl_geog['place_points'] = None
empl_geog['category_list'] = None

#Iterate through all LSOAs and grab OSM building counts and subcategories - add to empl_geog dataframe

for index, row in empl_geog.iterrows():

    #Status report
    if (index % 20 == 0):
        print(index)
    
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs=lsoa_geo.crs)

    #All Buildings - OSM
    filtered_osm_buildings = osm_all_buildings[osm_all_buildings.geometry.intersects(filter_geom.unary_union)]

    #Get subcategories based on tags
    commercial_buildings = osm_all_buildings[osm_all_buildings['building_type'] == 'commercial']
    office_buildings = osm_all_buildings[osm_all_buildings['building_type'] == 'office']
    retail_buildings = osm_all_buildings[osm_all_buildings['building_type'] == 'retail']
    residential_buildings = osm_all_buildings[osm_all_buildings['building_type'] == 'residential']

    #Subcategories of each building
    filtered_commercial_buildings = commercial_buildings[commercial_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_office_buildings = office_buildings[office_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_retail_buildings = retail_buildings[retail_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_residential_buildings = residential_buildings[residential_buildings.geometry.intersects(filter_geom.unary_union)]

    #Multipolygon of each LSOA's (OSM) buildings - category separated
    combined_multipolygon = filtered_osm_buildings.geometry.unary_union
    combined_commercial_multipolygon = filtered_commercial_buildings.geometry.unary_union
    combined_office_polygon = filtered_office_buildings.geometry.unary_union
    combined_retail_polygon = filtered_retail_buildings.geometry.unary_union
    combined_residential_polygon = filtered_residential_buildings.geometry.unary_union
    
    #All Places
    filtered_places = overture_places[overture_places.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places['category'].dropna().tolist()
    

    #Add back to Dataframe
    empl_geog.at[index, 'num_buildings'] = len(filtered_osm_buildings)
    
    empl_geog['num_retail_buildings'] = len(filtered_retail_buildings)
    empl_geog['num_residential_buildings'] = len(filtered_residential_buildings)
    empl_geog['num_commercial_buildings'] = len(filtered_commercial_buildings)
    empl_geog['num_office_buildings'] = len(filtered_office_buildings)
    
    empl_geog.at[index, 'num_places'] = len(filtered_places)
    
    empl_geog.at[index, 'building_poly'] = combined_multipolygon
    empl_geog.at[index, 'commercial_building_poly'] = combined_commercial_multipolygon
    empl_geog.at[index, 'retail_building_poly'] = combined_office_polygon
    empl_geog.at[index, 'office_building_poly'] = combined_retail_polygon
    empl_geog.at[index, 'residential_building_poly'] = combined_residential_polygon

    
    empl_geog.at[index, 'place_points'] = combined_multipoint
    empl_geog.at[index, 'category_list'] = category_list
    
#Check
empl_geog.head()


#Save to Dataframe on Desktop

#Convert multipolygons to WKT for export
empl_geog['building_poly'] = empl_geog['building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog['commercial_building_poly'] = empl_geog['commercial_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog['retail_building_poly'] = empl_geog['retail_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog['office_building_poly'] = empl_geog['office_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog['residential_building_poly'] = empl_geog['residential_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

empl_geog['place_points'] = empl_geog['place_points'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

# Convert category_list to string for export
empl_geog['category_list'] = empl_geog['category_list'].apply(lambda x: str(x) if x else None)

#Export
empl_geog.to_file("data/bham_data/empl_geog.geojson", driver="GeoJSON")


# Set back to Geometries for use in plot

# WKT strings back to Shapely geometries
empl_geog['building_poly'] = empl_geog['building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['commercial_building_poly'] = empl_geog['commercial_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['retail_building_poly'] = empl_geog['retail_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['office_building_poly'] = empl_geog['office_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog['residential_building_poly'] = empl_geog['residential_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

empl_geog['place_points'] = empl_geog['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)


#Get building subset - Birmingham 138A

#Code from earlier - isolate Birmingham 138A
lsoa = empl_geog[empl_geog['LSOA11NM_x'] == 'Birmingham 138A']
geom = lsoa['geometry'].iloc[0]
filter_geom = gpd.GeoSeries([geom], crs=lsoa_geo.crs)

#Get buildings
lsoa_all_buildings = gpd.GeoDataFrame(lsoa, geometry='building_poly')
lsoa_commercial_buildings = gpd.GeoDataFrame(lsoa, geometry='commercial_building_poly')
lsoa_office_buildings = gpd.GeoDataFrame(lsoa, geometry='office_building_poly')
lsoa_residential_buildings = gpd.GeoDataFrame(lsoa, geometry='residential_building_poly')
lsoa_retail_buildings = gpd.GeoDataFrame(lsoa, geometry='retail_building_poly')

# Plot OSM buildings
fig, ax = plt.subplots(figsize=(20, 20))
lsoa_all_buildings.plot(ax=ax, color='blue', edgecolor='black')
lsoa_commercial_buildings.plot(ax=ax, color='orange', edgecolor='black')
lsoa_office_buildings.plot(ax=ax, color='red', edgecolor='black')
lsoa_residential_buildings.plot(ax=ax, color='green', edgecolor='black')
lsoa_retail_buildings.plot(ax=ax, color='purple', edgecolor='black')

filter_geom.boundary.plot(ax=ax, color='red', linewidth=2)

plt.savefig('Plots/Birmingham_138A.png')


#Get all Birmingham

# Plot OSM buildings
fig, ax = plt.subplots(figsize=(20, 20))
osm_all_buildings.plot(ax=ax, legend=True, color='blue', edgecolor='black')
ax.set_axis_off()
plt.title('All OSM Birmingham Buildings', fontsize=16, fontweight='bold')

plt.savefig('Plots/Birmingham_All.png')








# Encode POI categories in data - using One-Hot encoding

#Make sure category list is a list
empl_geog['category_list'] = empl_geog['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

#Explode category lists
empl_geog_exploded = empl_geog.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories = encoder.fit_transform(empl_geog_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_exploded = pd.concat([empl_geog_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_encoded = empl_geog_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Store category columns
category_columns = empl_geog_encoded.columns[1:]

empl_geog_encoded.head()


# Geometric Features Extraction (copied from earlier but includes building types now)

#I'm going to exclude num polygons, average perimeter, and total perimeter cause they're not so helpful (cause Collinearity matrix shows its very close to count)

def extract_multipolygon_features(multipolygon, lsoa_geometry):
    if isinstance(multipolygon, MultiPolygon):
        polygons = list(multipolygon.geoms)
    elif isinstance(multipolygon, Polygon):
        polygons = [multipolygon]
    else:
        return pd.Series({
            'total_area': 0,
            'avg_building_area': 0,
            'lsoa_area_ratio': 0,
        })

    num_polygons = len(polygons)
    areas = [polygon.area for polygon in polygons]
    total_area = sum(areas)
    avg_building_area = total_area / num_polygons if num_polygons > 0 else 0

    #Built-up area ratio could result in a div by zero error if there are no buildings in an LSOA so this logic has to be included here
    try:
        lsoa_area_ratio = total_area / lsoa_geometry.area
    except ZeroDivisionError:
        lsoa_area_ratio = 0

    return pd.Series({
        'total_area': total_area,
        'lsoa_area_ratio': lsoa_area_ratio,
        'avg_building_area': avg_building_area,
    })



# All Buildings
all_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['building_poly'], row['geometry']),
    axis=1
).add_prefix('all_')

# Residential
residential_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['residential_building_poly'], row['geometry']),
    axis=1
).add_prefix('residential_')

# Commercial
commercial_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['commercial_building_poly'], row['geometry']),
    axis=1
).add_prefix('commercial_')

# Office
office_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['office_building_poly'], row['geometry']),
    axis=1
).add_prefix('office_')

# Retail
retail_buildings_geometry_features = empl_geog.apply(
    lambda row: extract_multipolygon_features(row['retail_building_poly'], row['geometry']),
    axis=1
).add_prefix('retail_')

# Combine all geometry features into one DataFrame
all_geom_features = pd.concat([
    all_buildings_geometry_features,
    residential_buildings_geometry_features,
    commercial_buildings_geometry_features,
    office_buildings_geometry_features,
    retail_buildings_geometry_features
], axis=1, ignore_index=False)

# Capture Column Names
geo_features = all_geom_features.columns

# Display the first 50 rows
all_geom_features.head(50)


# Combine Datasets

empl_geog['place_points'] = empl_geog['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

data_with_geom = pd.concat([empl_geog, all_geom_features], axis=1)
all_data = pd.concat([data_with_geom, empl_geog_encoded], axis=1)

all_data.head()





# create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(category_columns)
target = 'total_employment'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data[features], all_data[target], test_size=0.2, random_state=3)

# Save results and fit
automl = AutoML(results_path="automl_results_bham/basic_building_categories/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data[features])
geometries = all_data.loc[all_data[target].index, 'geometry']

results_basic_building = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data[target],
    'predicted': predictions_all,
})






#Filter to quality places only
quality_places = overture_places[overture_places['confidence'] >= 0.6]
quality_places.head()


#Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Generating a version of empl_geog that has only quality places - I'm using my data_with_geom file as a starting point
empl_geog_quality_places = data_with_geom

#Copy back quality places using logic from DataCleaning.ipynb

empl_geog_quality_places['category_list'] = None

for index, row in empl_geog.iterrows():
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs='EPSG:4326')

    #All Places
    filtered_places = quality_places[quality_places.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places['category'].dropna().tolist()

    #Add back to Dataframe
    empl_geog_quality_places.at[index, 'category_list'] = category_list

empl_geog_quality_places.head()


# One-Hot Encoding - yet again

#Explode category lists
empl_geog_quality_places_exploded = empl_geog_quality_places.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories_quality = encoder.fit_transform(empl_geog_quality_places_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories_quality, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_quality_places_exploded = pd.concat([empl_geog_quality_places_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_quality_places_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_quality_places_encoded = empl_geog_quality_places_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Save columns of interest for analysis

quality_categories = empl_geog_quality_places_encoded.columns[1:]

empl_geog_quality_places_encoded.head()



#Rejoin with all data

all_data_quality_places = pd.merge(empl_geog_quality_places, empl_geog_quality_places_encoded, on='LSOA11CD')

all_data_quality_places.head()





# Collapse Categories (Also Copied from DataExploration)

# Automated Condensing of Column Types

# List all columns

all_columns = empl_geog_encoded.columns.tolist()

# I'll group them by 'suffix' to find commonalities
suffix_groups = {}

# Iterate through columns to identify suffixes
for column in all_columns:
    lower_column = column.lower()
    parts = lower_column.split('_')
    if len(parts) > 1:
        suffix = parts[-1]
        if suffix in suffix_groups:
            suffix_groups[suffix].append(column)
        else:
            suffix_groups[suffix] = [column]
    else:
        # Handle columns without suffixes
        suffix_groups['other'] = suffix_groups.get('other', []) + [column]

# Create a list of DataFrames to concatenate
concatenated_dfs = []

# Iterate through suffix groups and aggregate columns
for suffix, columns in suffix_groups.items():
    if columns:
        if len(columns) == 1:
            # Preserve col name if only one col with suffix
            new_column_name = columns[0].replace('category_list_', '')
            concatenated_dfs.append(empl_geog_encoded[columns].rename(columns={columns[0]: new_column_name}))
        else:
            # Aggregate columns with more than one column in the group
            new_column_name = f'all_{suffix}'
            concatenated_dfs.append(empl_geog_encoded[columns].sum(axis=1).rename(new_column_name))

# Concatenate all DataFrames
condensed_categories = pd.concat(concatenated_dfs, axis=1)

# Print summary information (groups only)
for suffix, columns in suffix_groups.items():
    num_columns = len(columns)
    if num_columns > 1:
        print(f'Number of {suffix.capitalize()} Categories: {num_columns}')

condensed_categories.head()


#Remove all categories that have less than five in Birmingham (arbitrary cutoff for now)

column_sums = condensed_categories.iloc[:, 1:].sum()
columns_to_drop = column_sums[column_sums < 5].index.tolist()
print(columns_to_drop)

condensed_categories = condensed_categories.drop(columns=columns_to_drop)

#Store new categories
condensed_category_columns = condensed_categories.columns[1:]

condensed_categories.head()


# Join Condensed Version with data (Same Logic as in EarlyModels.ipynb and above)

all_data_cleaned = pd.merge(data_with_geom, condensed_categories, on='LSOA11CD')

all_data_cleaned.head()






# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'total_employment'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

# Save results and fit
automl = AutoML(results_path="automl_results_bham/raw_employment_category_cleaned/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_raw_employment_category_cleaned = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})






# Create a new column Employment Density and create new model to target that
all_data_cleaned['employment_density'] = all_data_cleaned['total_employment'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area



# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=42)

# Save results and fit
automl = AutoML(results_path="automl_results_bham/employment_density_category_cleaned/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_empl_density = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})





# Use office work cols from previous analysis

office_work_cols = [
    '42 : Civil engineering',
    '58 : Publishing activities',
    '59 : Motion picture, video and television programme production, sound recording and music publishing activities',
    '60 : Programming and broadcasting activities',
    '61 : Telecommunications',
    '62 : Computer programming, consultancy and related activities',
    '63 : Information service activities',
    '64 : Financial service activities, except insurance and pension funding',
    '65 : Insurance, reinsurance and pension funding, except compulsory social security',
    '66 : Activities auxiliary to financial services and insurance activities',
    '68 : Real estate activities',
    '69 : Legal and accounting activities',
    '70 : Activities of head offices; management consultancy activities',
    '71 : Architectural and engineering activities; technical testing and analysis',
    '72 : Scientific research and development',
    '73 : Advertising and market research',
    '74 : Other professional, scientific and technical activities',
    '77 : Rental and leasing activities',
    '78 : Employment activities',
    '79 : Travel agency, tour operator and other reservation service and related activities',
    '80 : Security and investigation activities',
    '82 : Office administrative, office support and other business support activities',
    '84 : Public administration and defence; compulsory social security'
]

# Create a new Office Work Total Column :
all_data_cleaned['office_total_employment'] = all_data_cleaned[office_work_cols].sum(axis=1)

# And an Office Work Density Column
all_data_cleaned['office_employment_density'] = all_data_cleaned['office_total_employment'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area

all_data_cleaned.head()



# Run the model

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'office_employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

# Save results and fit
automl = AutoML(results_path="automl_results_bham/office_employment_density_category_cleaned/", mode='Explain')
# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_density_cleaned = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})


# Performance Model Raw Employment

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'total_employment'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

automl = AutoML(
    results_path ='automl_results_bham/employment_cleaned_perform/',
    algorithms=["CatBoost", "Xg boost", "Random Forest"],
    model_time_limit=1*60,
    start_random_models=5,
    hill_climbing_steps=3,
    top_models_to_improve=3,
    features_selection=False,
    stack_models=True,
    train_ensemble=True,
    explain_level=1,
    validation_strategy={
        "validation_type": "kfold",
        "k_folds": 4,
        "shuffle": False,
        "stratify": True,
    }
)

# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_cleaned_perform = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})


# Performance Model

# Create training and testing data
features = ['num_buildings', 'num_places', 'population'] + list(geo_features) + list(condensed_category_columns)
target = 'office_employment_density'

# Split the dataset - 80/20 train test
X_train, X_test, y_train, y_test = train_test_split(all_data_cleaned[features], all_data_cleaned[target], test_size=0.2, random_state=3)

automl = AutoML(
    results_path ='automl_results_bham/office_employment_density_category_cleaned_perform/',
    algorithms=["CatBoost", "Xgboost", "Random Forest"],
    model_time_limit=1*60,
    start_random_models=5,
    hill_climbing_steps=3,
    top_models_to_improve=3,
    golden_features=True,
    features_selection=False,
    stack_models=True,
    train_ensemble=True,
    explain_level=0,
    validation_strategy={
        "validation_type": "kfold",
        "k_folds": 4,
        "shuffle": False,
        "stratify": True,
    }
)

# fit the model
automl.fit(X_train, y_train)

predictions = automl.predict(X_test)
r2 = r2_score(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f'R^2 Score: {r2}')
print(f'RMSE: {rmse}')

#Save results for plotting
predictions_all = automl.predict(all_data_cleaned[features])
geometries = all_data_cleaned.loc[all_data_cleaned[target].index, 'geometry']

results_office_density_cleaned_perform = pd.DataFrame({
    'geometry': geometries,
    'observed': all_data_cleaned[target],
    'predicted': predictions_all,
})





# Birmingham Residual Plots

# Basic Model

results_basic_building = gpd.GeoDataFrame(results_basic_building, geometry='geometry')
results_basic_building['residual'] = results_basic_building['observed'] - results_basic_building['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_basic_building.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - Birmingham Basic Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_bham/basic_model_residuals.png')
plt.close(fig)


# Category Cleaned

results_raw_employment_category_cleaned = gpd.GeoDataFrame(results_raw_employment_category_cleaned, geometry='geometry')
results_raw_employment_category_cleaned['residual'] = results_raw_employment_category_cleaned['observed'] - results_raw_employment_category_cleaned['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_raw_employment_category_cleaned.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - Birmingham Category Cleaned Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_bham/category_cleaned_model_residuals.png')
plt.close(fig)


# Category Cleaned Employment Density

results_empl_density = gpd.GeoDataFrame(results_empl_density, geometry='geometry')
results_empl_density['residual'] = results_empl_density['observed'] - results_empl_density['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_empl_density.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - Birmingham Employment Density Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_bham/density_model_residuals.png')
plt.close(fig)

# Category Cleaned Employment Density Office Employment

results_office_density_cleaned = gpd.GeoDataFrame(results_office_density_cleaned, geometry='geometry')
results_office_density_cleaned['residual'] = results_office_density_cleaned['observed'] - results_office_density_cleaned['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_office_density_cleaned.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - Birmingham Office Employment Density Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_bham/office_density_model_residuals.png')
plt.close(fig)

# Performance Model

results_office_cleaned_perform = gpd.GeoDataFrame(results_office_cleaned_perform, geometry='geometry')
results_office_cleaned_perform['residual'] = results_office_cleaned_perform['observed'] - results_office_cleaned_perform['predicted']

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
results_office_cleaned_perform.plot(ax=ax, column='residual', legend=True, cmap='viridis')
plt.title('Residuals - Birmingham Office Employment Density Model')
ax.set_axis_off()
plt.savefig('Plots/from_code/residuals_bham/employment_model_performance_residuals.png')
plt.close(fig)


# General Data Plots (Not from Model)

# Employment Count
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='total_employment', legend=True, cmap='viridis')
plt.title('Employment Count Map of Birmingham LSOAs')
ax.set_axis_off()
plt.title('Employment Count Map of Birmingham LSOAs')
plt.savefig('Plots/from_code/stats_bham/employment_count_map_bham.png')
plt.close(fig)

# Employment Density
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='employment_density', legend=True, cmap='viridis')
plt.title('Employment Density Map of Birmingham LSOAs')
ax.set_axis_off()
plt.title('Employment Density Map of Birmingham LSOAs')
plt.savefig('Plots/from_code/stats_bham/employment_density_map_bham.png')
plt.close(fig)

# Office Employment Density
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='office_employment_density', legend=True, cmap='viridis')
plt.title('Office Employment Density Map of Birmingham LSOAs')
ax.set_axis_off()
plt.title('Office Employment Density Map of Birmingham LSOAs')
plt.savefig('Plots/from_code/stats_bham/office_employment_density_map_bham.png')
plt.close(fig)


# Overture POI Density
all_data_cleaned['poi_density'] = all_data_cleaned['num_places'] / (all_data_cleaned['geometry']).to_crs("EPSG:27700").area

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='poi_density', legend=True, cmap='viridis')
plt.title('Overture POI Density Map of Birmingham LSOAs')
ax.set_axis_off()
plt.title('Overture POI Density Map of Birmingham LSOAs')
plt.savefig('Plots/from_code/stats_bham/poi_density_map_bham.png')
plt.close(fig)

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
all_data_cleaned.plot(ax=ax, column='num_places', legend=True, cmap='viridis')
plt.title('Overture POI Count Map of Birmingham LSOAs')
ax.set_axis_off()
plt.title('Overture POI Count Map of Birmingham LSOAs')
plt.savefig('Plots/from_code/stats_bham/poi_count_map_bham.png')
plt.close(fig)

